---
params:
  localData: ""
title: 'Processing report: NZ GREEN Grid project 1 minute electricity
  power data'
author: 'Ben Anderson (b.anderson@soton.ac.uk, `@dataknut`)'
date: 'Last run at: `r Sys.time()`'
output:
  html_document:
    code_folding: hide
    fig_caption: true
    keep_md: true
    number_sections: true
    self_contained: no
    toc: true
    toc_float: true
    toc_depth: 2
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 2
bibliography: '`r path.expand("~/bibliography.bib")`'
---

```{r knitrSetup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # by default turn off code echo
options(knitr.table.format = 'markdown') # try to fix the tables issue (seems to be pushing html into latex)
```


```{r codeSetup, include=FALSE}

# Set start time ----
startTime <- proc.time()


# Load nzGREENGrid package ----
library(nzGREENGridDataR) # local utilities

nzGREENGridDataR::setup()

# Set grid spy data paths etc from file ----
source(paste0(ggrParams$projLoc, "/dataProcessing/gridSpy/gSpyParams.R"))

# Packages needed in this .Rmd file ----
rmdLibs <- c("data.table", # data munching
             "ggplot2", # for fancy graphs
             "readr", # for reading & parsing .csv files
             "knitr", # for kable
             "stringr", # for str_wrap for long labels on plots
             "kableExtra" # for extra kable
)
# load them
nzGREENGrid::loadLibraries(rmdLibs)

# Local parameters ----

b2Kb <- 1024 #http://whatsabyte.com/P1/byteconverter.htm
b2Mb <- 1048576
plotLoc <- paste0(ggrParams$projLoc, "/reports/plots/")
statsLoc <- paste0(ggrParams$projLoc, "/checkStats/")
  
# Local functions ----


```

\newpage

# About

## Report circulation:

 * Public - this report is intended to accompany the data release.
 
## License

```{r ccby license, child=ggrParams$licenseCCBY}
```
 
## Citation

If you wish to use any of the material from this report please cite as:

 * Anderson, B. (`r 1900 + as.POSIXlt(Sys.Date())$year`) Processing report: NZ GREEN Grid project 1 minute electricity  power data, `r ggrParams$pubLoc`. 
 
> XX replace with UKDA DOI when available XX

## History

Code history is generally tracked via our github [repo](https://github.com/dataknut/nzGREENGridDataR):

 * [Report history](https://github.com/dataknut/nzGREENGridDataR/commits/master/dataProcessing/gridSpy/buildGridSpy1mReport.Rmd)
 * [General issues](https://github.com/dataknut/nzGREENGridDataR/issues)
 
## Requirements:

This report uses data quality statistics produced when processing the original grid spy 1 minute data downloads using https://github.com/dataknut/nzGREENGridDataR/blob/master/dataProcessing/gridSpy/processGridSpy1mData.R.

## Support

```{r generic support, child=ggrParams$supportGeneric}
```
 
\newpage

# Introduction

```{r generic sample, child=ggrParams$sampleGeneric}
```
 
This report provides summary data quality statistics for the original GREEN grid Grid Spy household power demand monitoring data. This data was used to create a derived 'safe' dataset using the code in the `r ggrParams$repo` repository.

# Original Data: Quality checks

The original data files files are stored on `r ggrParams$otagoHCS`. The data used to generate this report was at:

 * `r gSpyParams$gSpyInPath`

```{r getCompleteFileList}
# get file list 
fListCompleteDT <- data.table::as.data.table(readr::read_csv(gSpyParams$fListAll))

# for use below
nFiles <- nrow(fListCompleteDT)
nFilesNotLoaded <- nrow(fListCompleteDT[dateColName %like% "ignore"])
```

Data collection is ongoing and this section reports on the availability of data files collected up to the time at which the most recent safe file was created (`r file.mtime(gSpyParams$fListAll)`).

Overall we have `r tidyNum(nFiles)` files from `r tidyNum(uniqueN(fListCompleteDT$hhID))` households. However a large number of files (`r tidyNum(nFilesNotLoaded)` or `r round(100*(nFilesNotLoaded/nFiles),2)`%) have 1 of two file sizes (43 or 2751 bytes) and we have determined that they contain no data as the monitoring devices have either been removed (households have moved or withdrawn from the study) or data transfer has failed. We therefore flag these files as 'to be ignored'.

## Input data file quality checks

The following chart shows the distribution of _all_ files over time using their sizes. Note that white indicates the presence of small files which may not contain observations.

```{r allFileSizesPlot}
myCaption <- paste0("Data source: ", gSpyParams$gSpyInPath,
                    "\nUsing data received up to ", max(fListCompleteDT$fMDate))

plotDT <- fListCompleteDT[, .(nFiles = .N,
                              meanfSize = mean(fSize)), 
                          keyby = .(hhID, date = as.Date(fMDate))]

ggplot2::ggplot(plotDT, aes( x = date, y = hhID, 
                             fill = log(meanfSize))) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black") + 
  scale_x_date(date_labels = "%Y %b", date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 0.5, hjust = 0.5)) + 
  labs(title = "Mean file size of all grid spy data files received per day",
       caption = paste0(myCaption, 
                        "\nLog file size used as some files are full year data and so extremely large")
    
  )

ggplot2::ggsave(paste0(plotLoc, "gridSpyAllFileListSizeTilePlot.png"))
```
As we can see, relatively large files were donwloaded (manually) in June and October 2016 before an automated download process was implemented from January 2017. A final manual download appears to have taken place in early December 2017.

The following chart shows the same chart but _only_ for files which we think contain data.

```{r loadedFileSizesPlot}
plotDT <- fListCompleteDT[!is.na(dateFormat), .(nFiles = .N,
                              meanfSize = mean(fSize)), 
                          keyby = .(hhID, date = as.Date(fMDate))]

ggplot2::ggplot(plotDT, aes( x = date, y = hhID, fill = log(meanfSize))) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black") + 
  scale_x_date(date_labels = "%Y %b", date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) + 
  labs(title = "Mean file size of loaded grid spy data files received per day",
       caption = paste0(myCaption, 
                        "\nLog file size used as some files are full year data",
                        "\nFiles loaded if fsize > ", gSpyParams$gSpyFileThreshold, " bytes")
    
  )
ggplot2::ggsave(paste0(plotLoc, "/gridSpyLoadedFileListSizeTilePlot.png"))
```
As we can see this removes a large number of the autmatically downloaded files.

## Input date format checks

As noted above, the original data was downloaded in two ways:

 * Manual download of large samples of data. In this case the dateTime of the observation appears to have been stored in NZ time and appears also to have varying dateTime formats (d/m/y, y/m/d etc);
 * Automatic download of daily data. In this case the dateTime of the observation is stored as UTC

Resolving and cleaning these variations and uncertainties have required substantial effort and in some cases the date has had to be inferred from the file names.

The following table lists up to 10 of the 'date NZ' files which are set by default - do they look OK to assume the default dateFormat? Compare the file names with the dateExample...

```{r listDefaultDateFilesNZT, echo=TRUE}
# list default files with NZ time
aList <- fListCompleteDT[dateColName == "date NZ" & dateFormat %like% "default", 
                         .(file, fSize, dateColName, dateExample, dateFormat)]

cap <- paste0("First 10 (max) of ", nrow(aList), 
              " files with dateColName = 'date NZ' and default dateFormat")

knitr::kable(caption = cap, head(aList))
```

The following table lists up to 10 of the 'date UTC' files which are set by default - do they look OK to assume the default dateFormat? Compare the file names with the dateExample...

```{r listDefaultDateFilesUTC, echo=TRUE}
# list default files with UTC time
aList <- fListCompleteDT[dateColName == "date UTC" & dateFormat %like% "default", 
                         .(file, fSize, dateColName, dateExample, dateFormat)]

cap <- paste0("First 10 (max) of ", nrow(aList), 
              " files with dateColName = 'date UTC' and default dateFormat")

knitr::kable(caption = cap, head(aList, 10))
```

After final cleaning, the final date formats are:

```{r finalDateFormatTable}
# See what the date formats look like now
t <- fListCompleteDT[, .(nFiles = .N, 
                         meanFSizeKb = tidyNum(mean(fSize/b2Kb)),
                         minFSizeKb = tidyNum(min(fSize/b2Kb)),
                         maxFSizeKb = tidyNum(max(fSize/b2Kb)),
                         minFDate = min(fMDate), #Â may not make much sense
                         maxFDate = max(fMDate)), 
                     keyby = .(dateColName, dateFormat)]

knitr::kable(t,
             caption = "Number of files & min/max dates (as char) with given date column names by final imputed date format")
```

Results to note:

 * The non-loaded files only have 2 distinct file sizes, confirming that they are unlikely to contain useful data. 
 * There are a range of dateTme formats - these are fixed in the data cleaning process
 * Following detailed checks there are now `r nrow(fListCompleteDT[dateFormat == "ambiguous"])` files which are still labelled as having ambiguous dates;
 
# Processed Data: Quality checks

In this section we analyse the data files that have a file size > `r gSpyParams$gSpyFileThreshold` bytes and which have been used to create the safe data. Things to note:

 * As inidcated above, we assume that any files smaller than this value have no observations. This is based on:
     * Manual inspection of several small files
     * The identical (small) file sizes involved
 * We have had to deal with quite a lot of duplication some of which was caused by the different date formats, especially where they run through Daylight Savings Time (DST) changes.
 
The following table shows the number of files per household that are actually processed to make the safe version together with the min/max file save dates (not the observed data dates).

```{r filesToLoadTable, echo=TRUE}
#Â check files to load
t <- fListCompleteDT[dateColName %like% "date", .(nFiles = .N,
                       meanSize = mean(fSize),
                       minFileDate = min(fMDate),
                       maxFileDate = max(fMDate)), keyby = .(hhID)]

knitr::kable(caption = "Summary of household files to load", t)
```


## Circuit label checks

The following table shows the number of files for each household with different circuit labels. In theory each household should only have one set of unique circuit labels. If not:

 * some of the circuit labels for these households may have been changed during the data collection process;
 * some of the circuit labels may have character conversion errors which have changed the labels during the data collection process;
 * at least one file from one household has been saved to a folder containing data from a different household (unfortunately the raw data files do _not_ contain household IDs in the data or the file names which would enable checking/preventative filtering). This will be visible in the table if two households appear to share _exactly_ the same list of circuit labels.

Some or all of these may be true at any given time.

> NB: The table is only legible in the html version of this report because latex does a very bad job of wrapping table cell text. A version is saved in https://github.com/dataknut/nzGREENGridDataR/tree/master/checkStats for viewing in e.g. xl.

```{r circuitLabelCheck}
# get file list 
fLoadedListDT <- data.table::as.data.table(readr::read_csv(gSpyParams$fLoadedStats))

dt <- fLoadedListDT[!is.na(circuitLabels), 
                     .(nFiles = .N,
                       minObsDate = min(minDateTime), # helps locate issues in data
                       maxObsDate = max(maxDateTime),
                       minFileDate = min(mDateTime), # helps locate issues in files
                       maxFileDate = max(mDateTime),
                       nObs = sum(nObs)),
                     keyby = .(hhID,circuitLabels)] # ignore NA - it is files not loaded due to size thresholds

knitr::kable(caption = "Circuit labels list by number of files per household", dt)
ofile <- paste0(statsLoc, "circuitLabelsCheckTable.csv")
write.csv(dt, ofile)
```

Things to note:

 * rf_25 has an aditional unexpected "Incomer 1 - Uncontrolled$2757" circuit in some files but it's value is always NA so it has been ignored.
 * rf_46 had multiple circuit labels caused by apparent typos. These have been [re-labelled](https://github.com/dataknut/nzGREENGridDataR/issues/1) but note that this is the only household to have 13 circuits monitored.

Errors are easier to spot in the following plot where a hhID spans 2 or more circuit label sets.

```{r plotCircuitLabelIssuesAsTile, fig.height=8}

dt$newx = stringr::str_wrap(dt$circuitLabels, width = 40) #https://stackoverflow.com/questions/21878974/auto-wrapping-of-labels-via-labeller-label-wrap-in-ggplot2

ggplot2::ggplot(dt, aes(y = newx, x = hhID, 
                       fill = nObs)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 0.5)) + 
  theme(axis.text.y = element_text(size = 3)) + 
  theme(legend.position="bottom") +
  scale_fill_gradient(low="green", high = "red") +
  labs(title = "Circuit label counts for all loaded grid spy data",
       y = "Circuit label sets (as strings)",
       caption = paste0(myCaption,
                        "\nOnly files of size > ", 
                        gSpyParams$gSpyFileThreshold, " bytes loaded")
       
  )
ggplot2::ggsave(paste0(plotLoc, "gridSpyLoadedFileCircuitLabelsPlot.png"))
```

If the above plot and table flag a lot of errors then some re-naming of the circuit labels (column names) may be necessary. 

## Observations

The following plots show the number of observations per day per household. In theory we should not see:

 * dates before 2014 or in to the future. These may indicate:
    - date conversion errors;
 * more than 1440 observations per day. These may indicate:
    - duplicate time stamps - i.e. they have the same time stamps but different power (W) values or different circuit labels. These may be expected around the DST changes in April/September. These can be examined on a per household basis using the rf_xx_observationsRatioPlot.png plots to be found in the repo [checkPlots](https://github.com/dataknut/nzGREENGridDataR/tree/master/checkPlots) folder;
    - observations from files that are in the 'wrong' rf_XX folder and so are included in the 'wrong' household as 'duplicate' time stamps.

If present both of the latter may have been implied by the table above and would have evaded the de-duplication filter which simply checks each complete row against all others within it's consolidated household dataset (a _within household absolute duplicate_ check).

```{r loadedFilesObsPlots}
# get file list 
hhStatDT <- data.table::as.data.table(readr::read_csv(gSpyParams$hhStatsByDate))
myCaption <- paste0(myCaption,
                        "\nOnly files of size > ", 
                        gSpyParams$gSpyFileThreshold, " bytes loaded",
                    "\nNB: number of observations is a function of the number of circuits monitored",
                    "\n (see above) as well as missing data!")
  
# tile plot ----
ggplot2::ggplot(hhStatDT, aes( x = date, y = hhID, 
                               fill = nObs)) +
  geom_tile() +
  scale_fill_gradient(low = "red", high = "green") +
  scale_x_date(date_labels = "%Y %b", date_breaks = "6 months") +
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 0.5,
                                   hjust = 0.5)) + 
  labs(title = "N observations per household per day for all loaded grid spy data",
       caption = myCaption
       
  )
ggplot2::ggsave(paste0(plotLoc, "gridSpyLoadedFileNobsTilePlot.png"))

# point plot ----
ggplot2::ggplot(hhStatDT, aes( x = date, 
                               y = nObs, 
                               colour = hhID)) +
  geom_point() +
  scale_x_date(date_labels = "%Y %b", date_breaks = "6 months") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, 
                                   hjust = 0.5)) + 
  labs(title = "N observations per household per day for all loaded grid spy data",
       caption = myCaption
       
  )
ggplot2::ggsave(paste0(plotLoc, "gridSpyLoadedFileNobsPointPlot.png"))
```

The following table shows the min/max observations per day and min/max dates for each household. As above, we should not see:

 * dates before 2014 or in to the future (indicates date conversion errors);
 * less than 1440 observations per day (since we should have at least 1 circuit monitored for 24 * 60 = 1440 minutes);
 * non-integer counts of circuits as it suggests some circuit label errors or changes to the number of circuits monitored over time;
 * NA in any row (indicates date conversion errors).
 
 If we do see any of these then we still have data cleaning work to do!

```{r summaryTable}
# Stats table (so we can pick out the dateTime errors)
t <- hhStatDT[, .(minObs = min(nObs),
             maxObs = max(nObs), # should not be more than 1440, if so suggests duplicates
             meanN_Circuits =mean(nCircuits), #i.e. n circuits
             minDate = min(date),
             maxDate = max(date)),
         keyby = .(hhID)]

knitr::kable(caption = "Summary observation stats by hhID", t)
```


Finally we show the total number of households which we think are still sending data.

```{r liveDataHouseholds}
plotDT <- hhStatDT[, .(nHH = uniqueN(hhID)), keyby = .(date)]

# point plot ----
ggplot2::ggplot(plotDT, aes( x = date, y = nHH)) +
  geom_point() +
  scale_x_date(date_labels = "%Y %b", date_breaks = "6 months") +
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 0.5, 
                                   hjust = 0.5)) + 
  labs(title = "N live households per day for all loaded grid spy data",
       caption = myCaption
       
  )
ggplot2::ggsave(paste0(plotLoc, "gridSpyLiveHouseholdsToDate.png"))

```

# Runtime


```{r check runtime, include=FALSE}
t <- proc.time() - startTime

elapsed <- t[[3]]
```

Analysis completed in `r round(elapsed,2)` seconds ( `r round(elapsed/60,2)` minutes) using [knitr](https://cran.r-project.org/package=knitr) in [RStudio](http://www.rstudio.com) with `r R.version.string` running on `r R.version$platform`.

# R environment

R packages used:

 * base R - for the basics [@baseR]
 * data.table - for fast (big) data handling [@data.table]
 * lubridate - date manipulation [@lubridate]
 * ggplot2 - for slick graphics [@ggplot2]
 * readr - for csv reading/writing [@readr]
 * dplyr - for select and contains [@dplyr]
 * progress - for progress bars [@progress]
 * knitr - to create this document & neat tables [@knitr]
 * kableExtra - for extra neat tables [@kableExtra]
 * nzGREENGridDataR - for local NZ GREEN Grid project utilities

Session info:

```{r sessionInfo, echo=FALSE}
sessionInfo()
```

# References
